_target_: comde.comde_modules.seq2seq.GPT2SkillTranslator

seed: ${seed}

cfg:

  inseq_dim: ${language_dim} # Input sequence = sequence of skills
  lr: 1e-5
  startend_token_path: /home/jsw7460/mnt/comde_datasets/language_embeddings/bert_mappings/tokens/bert_base_start_end_tokens
  save_suffix: ${save_suffix}
  encoder_max_len: 200
  decoder_max_len: 7
  n_example: 1
  language_space: ${language_space.language_space}

  language_model: gpt2  # [gpt2, gpt2-large, bigscience/bloom-560m]

  transformer_cfg:
    input_dropout_prob: 0.1

    encoder_cfg:
      num_layers: 1
      input_dim: ${language_dim}
      num_heads: 8
      ff_dim: 32
      dropout_prob: 0.15
      use_bias: True
      max_len: ${prompt_learner.cfg.encoder_max_len}
      activation_fn: relu

    decoder_cfg:
      num_layers: 1
      input_dim: ${language_dim}
      num_heads: 4
      ff_dim: 32
      dropout_prob: 0.15
      use_bias: True
      max_len: ${prompt_learner.cfg.decoder_max_len}
      activation_fn: relu
